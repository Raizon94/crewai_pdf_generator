# ğŸ“„ CrewAI PDF Generator - ConfiguraciÃ³n Docker

## ğŸ¯ DescripciÃ³n

Este proyecto genera documentos PDF utilizando CrewAI y LLMs a travÃ©s de Ollama. La configuraciÃ³n Docker permite ejecutar la aplicaciÃ³n de manera consistente en diferentes sistemas operativos.

**Desarrollado por:** William Atef Tadrous y JuliÃ¡n Cussianovich  
**Asignatura:** AIN - Grupo 3CO11  
**Optimizado para:** gemma3:1b y gemma3:4b

## ğŸ—ï¸ Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Streamlit     â”‚    â”‚   CrewAI        â”‚    â”‚   Ollama        â”‚
â”‚   (Docker)      â”‚â—„â”€â”€â–ºâ”‚   (Docker)      â”‚â—„â”€â”€â–ºâ”‚   (Host)        â”‚
â”‚   Puerto 8501   â”‚    â”‚   Agentes AI    â”‚    â”‚   Puerto 11434  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Ventajas de esta arquitectura:**
- âœ… Ollama ejecutÃ¡ndose nativamente para mejor rendimiento de GPU
- âœ… AplicaciÃ³n containerizada para consistencia multiplataforma
- âœ… Modelos persistentes entre reinicios
- âœ… FÃ¡cil distribuciÃ³n y despliegue

## ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n

### 1. Prerrequisitos

#### Para todos los sistemas:
- [Docker Desktop](https://www.docker.com/products/docker-desktop/)
- [Ollama](https://ollama.com/) instalado nativamente

#### VerificaciÃ³n de Docker:
```bash
docker --version
docker compose version
```

#### VerificaciÃ³n de Ollama:
```bash
ollama --version
ollama list
```

### 2. ConfiguraciÃ³n de Ollama

#### Instalar y ejecutar Ollama:
```bash
# Descargar e instalar desde https://ollama.com/

# Ejecutar el servicio
ollama serve

# Descargar los modelos recomendados
ollama pull gemma3:1b
ollama pull gemma3:4b
```

#### Verificar que funciona:
```bash
curl http://localhost:11434/api/tags
```

### 3. ConfiguraciÃ³n del proyecto

#### Clonar y configurar:
```bash
# Navegar al directorio del proyecto
cd /ruta/a/crewai_pdf_generator

# Crear archivo de configuraciÃ³n
cp .env.example .env
```

#### Editar `.env` con tus credenciales:
```bash
# Obtener API key gratuita en https://serper.dev/
SERPER_API_KEY=tu_clave_serper_aqui

# ConfiguraciÃ³n Ollama (ya estÃ¡ configurada para localhost)
OLLAMA_HOST=localhost:11434
```

## ğŸ–¥ï¸ EjecuciÃ³n

### OpciÃ³n 1: Scripts automÃ¡ticos

#### En macOS/Linux:
```bash
chmod +x run-docker.sh
./run-docker.sh
```

#### En Windows:
```cmd
run-docker.bat
```

### OpciÃ³n 2: Comandos manuales

```bash
# Construir la imagen
docker compose build

# Ejecutar los servicios
docker compose up

# Para ejecutar en segundo plano
docker compose up -d
```

### Acceder a la aplicaciÃ³n:
ğŸŒ **http://localhost:8501**

## ğŸ› ï¸ Comandos Ãºtiles

### GestiÃ³n de contenedores:
```bash
# Ver logs en tiempo real
docker compose logs -f

# Parar los servicios
docker compose down

# Rebuild completo
docker compose down
docker compose build --no-cache
docker compose up
```

### DiagnÃ³stico:
```bash
# Verificar que Ollama estÃ© disponible desde Docker
docker run --rm curlimages/curl curl -f http://host.docker.internal:11434/api/tags

# Ver contenedores ejecutÃ¡ndose
docker compose ps

# Acceder al contenedor para debugging
docker compose exec crewai-app bash
```

## ğŸ”§ ConfiguraciÃ³n avanzada

### Variables de entorno disponibles:

| Variable | DescripciÃ³n | Valor por defecto |
|----------|-------------|-------------------|
| `SERPER_API_KEY` | API key para bÃºsquedas web | Requerido |
| `OLLAMA_HOST` | Host de Ollama | `localhost:11434` |
| `STREAMLIT_SERVER_PORT` | Puerto de Streamlit | `8501` |

### Para Linux con Docker:
```bash
# En .env, cambiar:
OLLAMA_HOST=172.17.0.1:11434
```

### Para ejecutar en puerto diferente:
```bash
# En docker-compose.yml, cambiar:
ports:
  - "3000:8501"  # Acceder en http://localhost:3000
```

## ğŸ“ Estructura de archivos

```
â”œâ”€â”€ app.py                 # AplicaciÃ³n principal Streamlit
â”œâ”€â”€ Dockerfile             # Imagen Docker de la aplicaciÃ³n
â”œâ”€â”€ docker-compose.yml     # OrquestaciÃ³n de servicios
â”œâ”€â”€ requirements.txt       # Dependencias Python
â”œâ”€â”€ .env.example          # Plantilla de configuraciÃ³n
â”œâ”€â”€ run-docker.sh         # Script de ejecuciÃ³n (Unix)
â”œâ”€â”€ run-docker.bat        # Script de ejecuciÃ³n (Windows)
â”œâ”€â”€ agents/               # Agentes CrewAI
â”œâ”€â”€ flows/                # Flujos de trabajo
â”œâ”€â”€ tools/                # Herramientas
â”œâ”€â”€ utils/                # Utilidades
â”œâ”€â”€ output/               # PDFs generados (persistente)
â””â”€â”€ temp/                 # Archivos temporales
```

## ğŸš¨ SoluciÃ³n de problemas

### Error: "No se encontraron modelos Ollama"
```bash
# Verificar que Ollama estÃ© ejecutÃ¡ndose
ollama serve

# Verificar modelos instalados
ollama list

# Descargar modelo si no existe
ollama pull gemma3:4b
```

### Error: "Connection refused to localhost:11434"
```bash
# En Windows/Mac, verificar que Docker pueda acceder al host
ping host.docker.internal

# En Linux, cambiar en .env:
OLLAMA_HOST=172.17.0.1:11434
```

### Error: "SERPER_API_KEY not configured"
```bash
# Obtener API key gratuita en https://serper.dev/
# Editar .env con la clave obtenida
```

### La aplicaciÃ³n es muy lenta
- Usar `gemma3:1b` en lugar de `gemma3:4b` para mayor velocidad
- Verificar que tienes suficiente RAM disponible
- Cerrar otras aplicaciones que usen mucha memoria

## ğŸ“Š Monitoreo

### Ver uso de recursos:
```bash
# Docker stats
docker stats

# Logs de la aplicaciÃ³n
docker compose logs crewai-app

# Logs de Ollama (fuera de Docker)
ollama logs
```

## ğŸ”„ Actualizaciones

### Actualizar la aplicaciÃ³n:
```bash
git pull origin main
docker compose down
docker compose build --no-cache
docker compose up
```

### Actualizar Ollama:
```bash
# Descargar nueva versiÃ³n desde https://ollama.com/
ollama --version
```

## ğŸ“ Soporte

Para problemas especÃ­ficos:
1. Verificar logs: `docker compose logs`
2. Probar Ollama directamente: `curl http://localhost:11434/api/tags`
3. Revisar la configuraciÃ³n en `.env`
4. Reiniciar servicios: `docker compose restart`

---

**Â¡Listo para generar PDFs con IA! ğŸš€**
